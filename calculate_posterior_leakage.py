# -*- coding: utf-8 -*-
import json
import numpy as np
import matplotlib.pyplot as plt
import os
from typing import Dict, Any, List, Optional
import argparse
from pathlib import Path

def resolve_path(p: str) -> str:
    """Resolve relative paths relative to this script's directory."""
    pth = Path(p)
    if pth.is_absolute():
        return str(pth.resolve())
    return str((Path(__file__).resolve().parent / pth).resolve())
# ===================== Experiment knobs (keep minimal) =====================
DEFAULT_VIOLATION_THRESHOLD = 5.0 # ä½ åªéœ€è¦ç»å¸¸æ”¹è¿™ä¸ªï¼ˆæˆ–ç”¨ CLI è¦†ç›–ï¼‰
SAVE_DIR_DEFAULT = "posterior_leakage_results"
DEFAULT_CLASS_METRIC_FILE = "distance_matrix_avg.json"

# NEW: force prior to uniform (can be overridden by CLI)
DEFAULT_UNIFORM_PRIOR = False

# NEW: merged summary output (like all_new.txt)
DEFAULT_MERGED_SUMMARY = True
DEFAULT_MERGED_SUMMARY_NAME = "all_summary_7_noiseonly.txt"

# numeric / plotting constants (usually don't need to change)
EPS = 1e-12
DIAG_FILL = 1e-12
HIST_BINS = 50

# NEW: make distribution milder when converting logits -> probs
SOFTMAX_T = 3.0  # try 2/3/5/10; larger => flatter (more "gentle")


def softmax_temp(z: np.ndarray, T: float = 1.0, axis: int = -1) -> np.ndarray:
    """Temperature softmax. T>1 flattens, T<1 sharpens. Supports vectors/matrices."""
    if T is None:
        T = 1.0
    T = float(T)
    if T <= 0:
        raise ValueError("Temperature T must be > 0")

    z = np.asarray(z, dtype=float) / T
    z = z - np.max(z, axis=axis, keepdims=True)  # stable
    e = np.exp(z)
    return e / np.sum(e, axis=axis, keepdims=True)


# ===================== èšåˆä¸å¯¹é½ =====================
def aggregate_by_label_logits(logits: np.ndarray, labels: np.ndarray, T: float = SOFTMAX_T):
    """
    å°†åŒä¸€ label çš„å¤šä¸ªçª—å£çº§ logits ç›¸åŠ ï¼ˆâ‰ˆç‹¬ç«‹è¯æ®ç›¸ä¹˜ï¼‰ï¼Œ
    å† temperature-softmax å¾—åˆ°â€œå¯¹è±¡çº§â€æ¦‚ç‡åˆ†å¸ƒã€‚

    é‡è¦è¯´æ˜ï¼š
    - è¿™é‡Œè¿”å›çš„ probs æ˜¯ softmax_temp(sum(logits), T) çš„ç»“æœï¼ˆå¯¹è±¡çº§ï¼‰ã€‚
    - T>1 ä¼šè®©åˆ†å¸ƒæ›´â€œæ¸©å’Œ/å¹³å¦â€ï¼Œå‡å¼±æœ€å¤§ç±»çš„æ”¾å¤§æ•ˆåº”ã€‚
    è¿”å›ï¼šagg_probs (num_labels, C), agg_labels (num_labels,)
    """
    labels = labels.astype(int)
    uniq = np.unique(labels)
    agg_logits = []
    agg_labels = []
    for lb in uniq:
        L = logits[labels == lb]          # (k, C)
        summed = L.sum(axis=0)            # logits ç›¸åŠ 
        agg_logits.append(summed)
        agg_labels.append(lb)
    agg_logits = np.vstack(agg_logits)    # (num_labels, C)

    probs = softmax_temp(agg_logits, T=T, axis=1)
    return probs, np.array(agg_labels)


def aggregate_by_label_probs_mult(probs: np.ndarray, labels: np.ndarray, eps: Optional[float] = None):
    """
    å¤‡é€‰ï¼šå½“æ²¡æœ‰ logits æ—¶ï¼ŒæŠŠåŒä¸€ label çš„çª—å£çº§æ¦‚ç‡ç›¸ä¹˜ï¼ˆlog æ¦‚ç‡ç›¸åŠ ï¼‰å†å½’ä¸€åŒ–ã€‚
    è¿”å›ï¼šagg_probs (num_labels, C), agg_labels (num_labels,)
    """
    if eps is None:
        eps = EPS

    labels = labels.astype(int)
    uniq = np.unique(labels)
    P = np.clip(probs, eps, 1.0)
    agg_logp = []
    agg_labels = []
    for lb in uniq:
        block = P[labels == lb]           # (k, C)
        logp = np.log(block).sum(axis=0)  # æ¦‚ç‡ä¹˜ç§¯ â†’ log æ¦‚ç‡ç›¸åŠ 
        agg_logp.append(logp)
        agg_labels.append(lb)
    agg_logp = np.vstack(agg_logp)

    # log-sum-exp å½’ä¸€åŒ–
    m = np.max(agg_logp, axis=1, keepdims=True)
    ex = np.exp(agg_logp - m)
    agg_probs = ex / ex.sum(axis=1, keepdims=True)
    return agg_probs, np.array(agg_labels)


def align_by_label_after_aggregation(
    prior_probs: np.ndarray, prior_labels: np.ndarray,
    posterior_probs: np.ndarray, posterior_labels: np.ndarray
):
    """
    å‡è®¾ä¸¤ä¾§éƒ½å·²æŒ‰ label èšåˆä¸ºå¯¹è±¡çº§åˆ†å¸ƒã€‚
    å¯¹å…±åŒçš„ label æ’åºåå¯¹é½ï¼Œä¿è¯åŒä¸€ label åœ¨ A/B åŒä¸€è¡Œã€‚
    """
    common = np.intersect1d(np.unique(prior_labels), np.unique(posterior_labels))
    if common.size == 0:
        raise ValueError("No common labels after aggregation; cannot align prior/posterior.")

    prior_idx = {lb: i for i, lb in enumerate(prior_labels)}
    post_idx  = {lb: i for i, lb in enumerate(posterior_labels)}
    A, B, L = [], [], []
    for lb in sorted(common):
        A.append(prior_probs[prior_idx[lb]])
        B.append(posterior_probs[post_idx[lb]])
        L.append(lb)
    return np.vstack(A), np.vstack(B), np.array(L)


# ===================== Pairwise mPL è®¡ç®— =====================

def calculate_pairwise_posterior_leakage(
    prior_probs: np.ndarray,
    posterior_probs: np.ndarray,
    class_metric: np.ndarray = None,
    epsilon: Optional[float] = None,
    batch_size: int = 2048,
) -> Dict[str, Any]:
    """
    é€å¯¹ (i,j) ç±»åˆ«è®¡ç®— | log(B_i/B_j) - log(A_i/A_j) | / d_{i,j}

    å›ºå®šåªç»Ÿè®¡æ— åºå¯¹ i<jï¼ˆåªç®— (i,j)ï¼Œä¸é‡å¤ç®— (j,i)ï¼‰ã€‚

    æ€§èƒ½ä¼˜åŒ–ï¼š
    - ä»…åœ¨ä¸Šä¸‰è§’ pairs ä¸Šè®¡ç®—ï¼ˆä½¿ç”¨ np.triu_indicesï¼‰ï¼Œé¿å…æ„é€  CÃ—C çš„ diff çŸ©é˜µ
    - å¯¹å¯¹è±¡ç»´åº¦æŒ‰ batch å¤„ç†ï¼Œå‡å°‘ Python å¾ªç¯ä¸ä¸´æ—¶å¤§æ•°ç»„
    """
    if epsilon is None:
        epsilon = EPS

    assert prior_probs.shape == posterior_probs.shape, "prior/posterior ç»´åº¦ä¸ä¸€è‡´"
    N, C = prior_probs.shape

    A = np.clip(prior_probs, epsilon, 1.0)
    B = np.clip(posterior_probs, epsilon, 1.0)
    A = A / A.sum(axis=1, keepdims=True)
    B = B / B.sum(axis=1, keepdims=True)

    # åªå– i<j çš„ pair ç´¢å¼•ï¼ˆP = C*(C-1)/2ï¼‰
    i_idx, j_idx = np.triu_indices(C, k=1)
    P = int(i_idx.size)

    # å‡†å¤‡è·ç¦»åˆ†æ¯ï¼ˆåªå– pairs éƒ¨åˆ†ï¼‰
    if class_metric is None:
        denom = np.ones((P,), dtype=float)
    else:
        D = np.array(class_metric, dtype=float)
        assert D.shape == (C, C)
        denom = D[i_idx, j_idx].astype(float, copy=False)
        denom = np.where(denom <= 0, epsilon, denom)

    # é¢„è®¡ç®— logï¼Œé¿å…æ¯ä¸ªå¯¹è±¡é‡å¤ np.log
    logA = np.log(A)
    logB = np.log(B)

    # batch åŒ–è®¡ç®—ï¼Œé¿å… N æ¬¡ Python å¾ªç¯ + é¿å…æ„é€  (C,C) çŸ©é˜µ
    if batch_size is None or batch_size <= 0:
        batch_size = N  # ä¸€æ¬¡æ€§ï¼ˆå¯èƒ½å å†…å­˜ï¼‰

    chunks: List[np.ndarray] = []
    for start in range(0, N, batch_size):
        end = min(N, start + batch_size)
        la = logA[start:end]  # (b, C)
        lb = logB[start:end]  # (b, C)

        # (b, P): åªåœ¨ pairs ä¸Šåšå·®
        delta = (lb[:, i_idx] - lb[:, j_idx]) - (la[:, i_idx] - la[:, j_idx])
        pl = np.abs(delta) / denom  # broadcast denom: (P,)
        chunks.append(pl.reshape(-1))

    arr = np.concatenate(chunks, axis=0) if chunks else np.array([], dtype=float)
    pairwise_pl_values = arr.tolist()

    stats = {
        'mean_pl': float(np.mean(arr)) if arr.size else 0.0,
        'std_pl': float(np.std(arr)) if arr.size else 0.0,
        'min_pl': float(np.min(arr)) if arr.size else 0.0,
        'max_pl': float(np.max(arr)) if arr.size else 0.0,
        'median_pl': float(np.median(arr)) if arr.size else 0.0,
        'total_counts': int(arr.size)
    }
    return {'pairwise_pl': pairwise_pl_values, 'statistics': stats}


# ===================== å¯è§†åŒ–ï¼ˆpairwise ç‰ˆæœ¬ï¼‰ =====================

def create_pl_distribution_plot(
    pl_values: List[float],
    save_path: str,
    dataset_comparison: str,
    violation_threshold: Optional[float] = None
):
    """æŠŠæ¯ä¸ª (i,j) çš„ PL å½“ä½œä¸€ä¸ªæ ·æœ¬/è®¡æ•°æ¥ç”»ç›´æ–¹å›¾"""
    if violation_threshold is None:
        violation_threshold = DEFAULT_VIOLATION_THRESHOLD

    arr = np.array(pl_values)
    if len(arr) == 0:
        print("No PL values to plot")
        return

    plt.figure(figsize=(12, 8))

    counts, bin_edges = np.histogram(arr, bins=HIST_BINS)
    bin_centers = (bin_edges[:-1] + bin_edges[1:]) / 2
    bin_width = bin_edges[1] - bin_edges[0]

    not_violated_mask = bin_centers <= violation_threshold
    violated_mask = bin_centers > violation_threshold

    plt.bar(bin_centers[not_violated_mask], counts[not_violated_mask],
            width=bin_width, alpha=0.7, edgecolor='black',
            color='steelblue', label='Not violated')

    if np.any(violated_mask):
        plt.bar(bin_centers[violated_mask], counts[violated_mask],
                width=bin_width, alpha=0.7, edgecolor='black',
                color='red', label='Violated')

    plt.axvline(violation_threshold, color='darkred', linestyle='-', linewidth=3,
                label=f'Violation threshold (Îµ = {violation_threshold})')
    plt.axvline(np.mean(arr), color='orange', linestyle='--', linewidth=2,
                label=f'Mean: {np.mean(arr):.4f}')

    violated_values = arr[arr > violation_threshold]
    violation_ratio = len(violated_values) / len(arr) * 100 if len(arr) > 0 else 0.0

    plt.xlabel('Pairwise Posterior Leakage (PL)', fontsize=12)
    plt.ylabel('Count (number of class pairs)', fontsize=12)
    plt.title(f'Posterior Leakage Distribution (pairwise)\n{dataset_comparison}\nViolation ratio: {violation_ratio:.1f}%',
              fontsize=14)
    plt.legend()
    plt.grid(True, alpha=0.3)
    plt.tight_layout()
    plt.savefig(save_path, dpi=300, bbox_inches='tight')
    plt.close()
    print(f"âœ… Pairwise PL distribution plot saved: {save_path}")


# ===================== ä¸»æµç¨‹ =====================

def analyze_posterior_leakage_between_datasets(
    prior_file: str,
    posterior_file: str,
    save_dir: str = SAVE_DIR_DEFAULT,
    class_metric: np.ndarray = None,
    violation_threshold: Optional[float] = None,
    uniform_prior: bool = DEFAULT_UNIFORM_PRIOR,
    batch_size: int = 2048,
):
    """
    åˆ†æä¸¤ä¸ªæ•°æ®é›†ä¹‹é—´çš„åéªŒæ¦‚ç‡æ³„éœ²ï¼ˆpairwise ç»Ÿè®¡ï¼‰ï¼š
    - çª—å£çº§ -> ï¼ˆæŒ‰ label èšåˆï¼‰å¯¹è±¡çº§æ¦‚ç‡
    - å¯¹åŒä¸€å¯¹è±¡çš„æ‰€æœ‰ç±»å¯¹ (i,j) äº§ç”Ÿä¸€ä¸ª PL å€¼ï¼ˆcount=1ï¼‰
    - ç›´æ–¹å›¾æŒ‰æ‰€æœ‰ pairwise PL å€¼ä½œå›¾
    """
    if violation_threshold is None:
        violation_threshold = DEFAULT_VIOLATION_THRESHOLD

    print(f"\n{'='*60}")
    print(f"åˆ†æåéªŒæ³„éœ²ï¼ˆpairwise æˆå¯¹èµ”ç‡å®šä¹‰ï¼‰")
    print(f"Prior (æœªåŠ å™ª): {prior_file}")
    print(f"Posterior (åŠ å™ª): {posterior_file}")
    print(f"{'='*60}\n")

    # åŠ è½½æ•°æ®
    with open(prior_file, 'r', encoding='utf-8') as f:
        prior_data = json.load(f)
    with open(posterior_file, 'r', encoding='utf-8') as f:
        posterior_data = json.load(f)

    prior_labels_raw = np.array(prior_data['labels'])
    posterior_labels_raw = np.array(posterior_data['labels'])

    # --- ä¼˜å…ˆä½¿ç”¨ logits åšâ€œè”åˆè§‚æµ‹â€èšåˆ ---
    # è‹¥æä¾› logitsï¼šä½¿ç”¨ softmax(sum logits) å¾—åˆ°å¯¹è±¡çº§æ¦‚ç‡ï¼›
    # å¦åˆ™ï¼šä½¿ç”¨è¾“å…¥çš„çª—å£çº§ probs åšæ¦‚ç‡ä¹˜ç§¯è¿‘ä¼¼èšåˆã€‚
    if 'logits' in prior_data and 'logits' in posterior_data:
        prior_logits_raw = np.array(prior_data['logits'])
        posterior_logits_raw = np.array(posterior_data['logits'])

        prior_probs_agg, prior_labels_agg = aggregate_by_label_logits(prior_logits_raw, prior_labels_raw, T=SOFTMAX_T)
        posterior_probs_agg, posterior_labels_agg = aggregate_by_label_logits(posterior_logits_raw, posterior_labels_raw, T=SOFTMAX_T)
        aggregation_method = "logits_sum_softmax_temp"
        print(f"å·²ä½¿ç”¨ logits èšåˆä¸ºå¯¹è±¡çº§æ¦‚ç‡ï¼ˆtemperature-softmax, T={SOFTMAX_T}ï¼‰ã€‚")
    else:
        prior_probs_raw = np.array(prior_data['probs'])
        posterior_probs_raw = np.array(posterior_data['probs'])

        prior_probs_agg, prior_labels_agg = aggregate_by_label_probs_mult(prior_probs_raw, prior_labels_raw)
        posterior_probs_agg, posterior_labels_agg = aggregate_by_label_probs_mult(posterior_probs_raw, posterior_labels_raw)
        aggregation_method = "prob_product_norm"
        print("æœªå‘ç° logitsï¼Œå·²ä½¿ç”¨æ¦‚ç‡ä¹˜ç§¯è¿‘ä¼¼èšåˆä¸ºå¯¹è±¡çº§æ¦‚ç‡ã€‚")

    # --- å¯¹é½ï¼ˆå¯¹è±¡çº§ï¼‰---
    prior_probs, posterior_probs, labels = align_by_label_after_aggregation(
        prior_probs_agg, prior_labels_agg,
        posterior_probs_agg, posterior_labels_agg
    )

    # NEW: override prior with uniform distribution (keep posterior unchanged)
    if uniform_prior:
        C_ = int(posterior_probs.shape[1])
        prior_probs = np.full((prior_probs.shape[0], C_), 1.0 / C_, dtype=float)
        print(f"âœ… å·²å°† prior å¼ºåˆ¶è®¾ä¸º uniform åˆ†å¸ƒï¼ˆæ¯ç±»=1/{C_}ï¼‰ï¼Œå…¶ä½™ä¿æŒä¸å˜ã€‚")

    print(f"å¯¹é½åå¯¹è±¡æ•°ï¼š{len(labels)}")
    print(f"ç±»åˆ«æ•°ï¼š{prior_probs.shape[1]}")

    # å›ºå®š pair æ¨¡å¼ï¼ˆåªç®— i<jï¼‰
    C = int(prior_probs.shape[1])
    pairs_per_object = (C * (C - 1)) // 2
    pair_mode = "unordered(i<j)"

    # è®¡ç®— pairwise åéªŒæ³„éœ²
    print("è®¡ç®— pairwise åéªŒæ³„éœ²ï¼ˆåŒä¸€å¯¹è±¡å†…æˆå¯¹ç±»èµ”ç‡å˜åŒ–ï¼‰...")
    pl_result = calculate_pairwise_posterior_leakage(
        prior_probs,
        posterior_probs,
        class_metric=class_metric,
        epsilon=EPS,
        batch_size=batch_size,
    )

    # åˆ›å»ºä¿å­˜ç›®å½•
    os.makedirs(save_dir, exist_ok=True)

    # æå–æ•°æ®é›†åç§°
    prior_name = os.path.basename(prior_file).replace('per_sample_probs_', '').replace('.json', '')
    posterior_name = os.path.basename(posterior_file).replace('per_sample_probs_', '').replace('.json', '')
    comparison_name = f"{prior_name}_vs_{posterior_name}"

    # ä¿å­˜è¯¦ç»†ç»“æœ
    detailed_result = {
        'comparison': {
            'prior_dataset': prior_name,
            'posterior_dataset': posterior_name,
            'num_aligned_objects': int(len(labels)),
            'num_classes': C,
            'aggregation': aggregation_method,
            'uniform_prior': bool(uniform_prior),
            'softmax_temperature_T': float(SOFTMAX_T) if aggregation_method == "logits_sum_softmax_temp" else None,
            'pair_mode': pair_mode,
            'pairs_per_object': int(pairs_per_object),
            'pairwise_total_counts': pl_result['statistics']['total_counts']
        },
        'pairwise_posterior_leakage': {
            'pairwise_pl': pl_result['pairwise_pl'],
            'statistics': pl_result['statistics']
        }
    }

    detailed_path = os.path.join(save_dir, f"{comparison_name}_pairwise_leakage_detailed.json")
    with open(detailed_path, 'w', encoding='utf-8') as f:
        json.dump(detailed_result, f, indent=2, ensure_ascii=False)
    print(f"âœ… è¯¦ç»†ç»“æœå·²ä¿å­˜: {detailed_path}")

    # åˆ›å»ºåˆ†å¸ƒå›¾ï¼ˆpairwiseï¼‰
    distribution_path = os.path.join(save_dir, f"{comparison_name}_pairwise_distribution.png")
    create_pl_distribution_plot(
        pl_result['pairwise_pl'],
        distribution_path,
        f"{prior_name} (prior) vs {posterior_name} (posterior)",
        violation_threshold=violation_threshold
    )

    # ç”Ÿæˆæ‘˜è¦
    stats = pl_result['statistics']
    summary_path = os.path.join(save_dir, f"{comparison_name}_pairwise_summary.txt")

    # NEW: build summary text once (for both per-run file and merged file)
    summary_lines = []
    summary_lines.append("Posterior Leakage Analysis Summary (Pairwise mPL)\n")
    summary_lines.append("=" * 50 + "\n")
    summary_lines.append(f"Prior Dataset: {prior_name}\n")
    summary_lines.append(f"Posterior Dataset: {posterior_name}\n")
    summary_lines.append(f"Aligned Objects: {len(labels)}\n")
    summary_lines.append(f"Number of Classes: {prior_probs.shape[1]}\n")
    summary_lines.append(f"Aggregation: {aggregation_method}\n")
    summary_lines.append(f"Uniform prior: {bool(uniform_prior)}\n")
    if aggregation_method == "logits_sum_softmax_temp":
        summary_lines.append(f"Softmax temperature T: {SOFTMAX_T}\n")
    summary_lines.append(f"Pair mode: {pair_mode}\n")
    summary_lines.append(f"Pairs per object: {pairs_per_object}\n\n")
    summary_lines.append("Pairwise Posterior Leakage Statistics:\n")
    summary_lines.append("-" * 30 + "\n")
    summary_lines.append(f"Total Counts (pairs across all objects): {stats['total_counts']}\n")
    summary_lines.append(f"Mean PL: {stats['mean_pl']:.6f}\n")
    summary_lines.append(f"Std PL: {stats['std_pl']:.6f}\n")
    summary_lines.append(f"Median PL: {stats['median_pl']:.6f}\n")
    summary_lines.append(f"Min PL: {stats['min_pl']:.6f}\n")
    summary_lines.append(f"Max PL: {stats['max_pl']:.6f}\n")
    summary_text = "".join(summary_lines)

    with open(summary_path, 'w', encoding='utf-8') as f:
        f.write(summary_text)

    print(f"âœ… æ‘˜è¦å·²ä¿å­˜: {summary_path}")

    # æ‰“å°ç»Ÿè®¡ä¿¡æ¯
    print(f"\n{'='*60}")
    print("åéªŒæ³„éœ²ç»Ÿè®¡ (Pairwise mPL):")
    print(f"{'='*60}")
    print(f"Counts: {stats['total_counts']}")
    print(f"å¹³å‡ PL: {stats['mean_pl']:.6f}")
    print(f"æ ‡å‡†å·® PL: {stats['std_pl']:.6f}")
    print(f"ä¸­ä½æ•° PL: {stats['median_pl']:.6f}")
    print(f"æœ€å° PL: {stats['min_pl']:.6f}")
    print(f"æœ€å¤§ PL: {stats['max_pl']:.6f}")
    print(f"{'='*60}\n")

    # NEW: attach summary text for merged output
    detailed_result["summary_text"] = summary_text
    return detailed_result


# ===================== CLI =====================

if __name__ == "__main__":
    # ===================== å¦‚ä½•è¿è¡Œï¼ˆç›´æ¥å¤åˆ¶å³å¯ï¼‰ =====================
    # 1) é»˜è®¤ï¼šä¸€æ¬¡è·‘å®Œ noise_1..noise_10ï¼ˆprior å›ºå®šä¸º budget_7_mask_abstract.jsonï¼‰
    #    python calculate_posterior_leakage.py
    #
    # 2) åªè·‘æŸä¸€ä¸ªï¼ˆä¾‹å¦‚ noise_3ï¼‰
    #    python calculate_posterior_leakage.py --noise-start 3 --noise-end 3
    #
    # 3) è·‘è‡ªå®šä¹‰èŒƒå›´ï¼ˆä¾‹å¦‚ noise_4..noise_8ï¼‰
    #    python calculate_posterior_leakage.py --noise-start 4 --noise-end 8
    #
    # 4) posterior æ–‡ä»¶åä¸ç¬¦åˆæ¨¡æ¿æ—¶ï¼šç›´æ¥ç»™åˆ—è¡¨
    #    python calculate_posterior_leakage.py --posterior-files budget_7_noise_3_abstract.json budget_7_noise_7_abstract.json
    #
    # 5) æŒ‡å®š prior / è¾“å‡ºç›®å½• / é˜ˆå€¼ï¼š
    #    python calculate_posterior_leakage.py --prior-file budget_7_mask_abstract.json --save-dir posterior_leakage_results --threshold 2.0
    # ===============================================================

    files = [
        'outputs/WikiActors/multi_7_noiseonly/eval_results_noise_0p001_abstract.json',
        'outputs/WikiActors/multi_7_noiseonly/eval_results_noise_3p0_abstract.json',
    ]

    parser = argparse.ArgumentParser(description="Pairwise posterior leakage analysis (i<j only)")
    parser.add_argument("--prior-file", default=files[0])
    parser.add_argument("--posterior-file", default=files[1])

    # NEW: å…è®¸æŒ‡å®šè¾“å‡ºç›®å½•ï¼ˆå¯é€‰ï¼›é»˜è®¤ SAVE_DIR_DEFAULTï¼‰
    parser.add_argument("--save-dir", default=SAVE_DIR_DEFAULT, help="Directory to save results (relative to this script if not absolute).")

    # æ‰¹é‡ posteriorï¼ˆä¸¤ç§æ–¹å¼äºŒé€‰ä¸€ï¼‰
    parser.add_argument(
        "--posterior-files",
        nargs="*",
        default=None,
        help="Optional: provide multiple posterior json files. If set, overrides --posterior-file/template/range.",
    )
    parser.add_argument(
        "--posterior-template",
        type=str,
        default="outputs/WikiActors/multi_7_noiseonly/eval_results_noise_{i}p0_abstract.json",

        # help='Optional: template to generate posterior files, e.g. "budget_2_clean_noise_{i}p0_abstract.json".',
    )
    parser.add_argument("--noise-start", type=int, default=1, help="Start index i for --posterior-template.")
    parser.add_argument("--noise-end", type=int, default=10, help="End index i for --posterior-template (inclusive).")

    parser.add_argument(
        "--threshold",
        dest="violation_threshold",
        type=float,
        default=DEFAULT_VIOLATION_THRESHOLD,
        help="Violation threshold for PL histogram coloring/ratio.",
    )

    # NEW: toggle uniform prior
    parser.add_argument("--uniform-prior", dest="uniform_prior", action="store_true",
                        help="Force prior distribution A to be uniform after alignment.")
    parser.add_argument("--original-prior", dest="uniform_prior", action="store_false",
                        help="Use prior distribution from file (after aggregation/alignment).")
    parser.set_defaults(uniform_prior=DEFAULT_UNIFORM_PRIOR)

    # NEW: merged summary options
    parser.add_argument("--merged-summary", dest="merged_summary", action="store_true",
                        help="Write all per-run summaries into one merged txt under --save-dir.")
    parser.add_argument("--no-merged-summary", dest="merged_summary", action="store_false",
                        help="Disable merged summary txt.")
    parser.set_defaults(merged_summary=DEFAULT_MERGED_SUMMARY)

    parser.add_argument("--merged-summary-name", type=str, default=DEFAULT_MERGED_SUMMARY_NAME,
                        help="Filename for merged summary txt (saved inside --save-dir).")

    parser.add_argument(
        "--batch-size",
        type=int,
        default=2048,
        help="Batch size over objects for pairwise PL computation. Larger is faster but uses more RAM.",
    )

    args = parser.parse_args()

    # NEW: ç»Ÿä¸€æŠŠç›¸å¯¹è·¯å¾„è§£æåˆ°â€œè„šæœ¬æ‰€åœ¨ç›®å½•â€ï¼Œä¿è¯ç›´æ¥è¿è¡Œç¨³å®š
    args.prior_file = resolve_path(args.prior_file)
    args.posterior_file = resolve_path(args.posterior_file)
    args.save_dir = resolve_path(args.save_dir)

    # å›ºå®šï¼šå°è¯•åŠ è½½é»˜è®¤ç±»åˆ«è·ç¦»çŸ©é˜µæ–‡ä»¶ï¼›ä¸å­˜åœ¨åˆ™é€€åŒ–ä¸º unit metric
    try:
        metric_path = resolve_path(DEFAULT_CLASS_METRIC_FILE)
        with open(metric_path, "r", encoding="utf-8") as f:
            distance_data = json.load(f)
        class_metric = np.array(distance_data["distance_matrix"])
        print(f"âœ… åŠ è½½ç±»åˆ«è·ç¦»çŸ©é˜µ: {class_metric.shape} ({metric_path})")
    except FileNotFoundError:
        print("âš ï¸ æœªæ‰¾åˆ°ç±»åˆ«è·ç¦»çŸ©é˜µï¼Œä½¿ç”¨é»˜è®¤å€¼ (å…¨1)")
        class_metric = None

    # ç»„è£… posterior åˆ—è¡¨ï¼ˆä¼˜å…ˆ --posterior-filesï¼›å¦åˆ™ç”¨ template + rangeï¼›å¦åˆ™é€€å›å•æ–‡ä»¶ï¼‰
    if args.posterior_files is not None and len(args.posterior_files) > 0:
        posterior_list = [resolve_path(p) for p in args.posterior_files]
        src = "--posterior-files"
    else:
        if args.noise_end < args.noise_start:
            raise ValueError("--noise-end must be >= --noise-start")
        posterior_list = [resolve_path(args.posterior_template.format(i=i)) for i in range(args.noise_start, args.noise_end + 1)]
        src = "--posterior-template/--noise-start/--noise-end"

        # ä¿æŒå…¼å®¹ï¼šå¦‚æœä½ æ˜¾å¼åªæƒ³è·‘å•ä¸ªæ–‡ä»¶ï¼Œä¹Ÿå¯ä»¥ç›´æ¥ä¼  --posterior-file
        default_template = "budget_7_noise_{i}_abstract.json"
        if (
            args.posterior_template == default_template
            and args.noise_start == 1
            and args.noise_end == 10
            and args.posterior_file != resolve_path(files[1])
        ):
            posterior_list = [args.posterior_file]
            src = "--posterior-file (single)"

    print(f"\nå°†æ‰¹é‡è®¡ç®— posterior æ–‡ä»¶ï¼ˆæ¥æº: {src}ï¼‰ï¼Œæ•°é‡={len(posterior_list)}")
    print("Posterior list preview:", posterior_list[:5], ("..." if len(posterior_list) > 5 else ""))

    ok, skipped = 0, 0

    # NEW: open merged summary file once (overwrite), append each run in order
    merged_f = None
    merged_path = None
    if args.merged_summary:
        os.makedirs(args.save_dir, exist_ok=True)
        merged_path = os.path.join(args.save_dir, args.merged_summary_name)
        merged_f = open(merged_path, "w", encoding="utf-8")

    try:
        for posterior_file in posterior_list:
            if not os.path.exists(posterior_file):
                print(f"âš ï¸ è·³è¿‡ä¸å­˜åœ¨çš„ posterior æ–‡ä»¶: {posterior_file}")
                skipped += 1
                continue

            result = analyze_posterior_leakage_between_datasets(
                prior_file=args.prior_file,
                posterior_file=posterior_file,
                save_dir=args.save_dir,
                class_metric=class_metric,
                violation_threshold=args.violation_threshold,
                uniform_prior=args.uniform_prior,
                batch_size=args.batch_size,
            )

            if merged_f is not None:
                merged_f.write(result.get("summary_text", ""))
                merged_f.write("\n\n")  # separate blocks like all_new.txt

            ok += 1
    finally:
        if merged_f is not None:
            merged_f.close()
            print(f"âœ… æ±‡æ€»æ‘˜è¦å·²ä¿å­˜: {merged_path}")

    print(f"\nğŸ‰ æ‰¹é‡åˆ†æå®Œæˆ: ok={ok}, skipped={skipped}")

# python calculate_posterior_leakage.py --original-prior